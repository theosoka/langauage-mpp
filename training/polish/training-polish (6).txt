Perceptron – najprostsza sieć neuronowa, składająca się z jednego bądź wielu niezależnych neuronów McCullocha-Pittsa, implementująca algorytm uczenia nadzorowanego klasyfikatorów binarnych. Perceptron jest funkcją, która potrafi określić przynależność parametrów wejściowych do jednej z dwóch klas, poprzez wskazanie czy coś należy czy nie do pierwszej klasy. Może być wykorzystywany tylko do klasyfikowania zbiorów liniowo separowalnych. Aby móc testować przynależność do więcej niż dwóch klas, należy użyć perceptronu z większą ilością neuronów, w którym klasy zakodowane są jako wyjścia perceptronu (dla danych testowych), w postaci bitów.
Działanie perceptronu polega na klasyfikowaniu danych pojawiających się na wejściu i ustawianiu stosownie do tego wartości wyjścia. Przed używaniem perceptron należy wytrenować, podając mu przykładowe dane na wejście i modyfikując w odpowiedni sposób wagi wejść i połączeń między warstwami neuronów, tak aby wynik na wyjściu przybierał pożądane wartości. Perceptrony mogą klasyfikować dane na zbiory, które są liniowo separowalne. Własność ta uniemożliwia na przykład wytrenowanie złożonego z jednego neuronu perceptronu, który wykonywałby logiczną operację XOR na wartościach wejść. Z matematycznego punktu widzenia wagi perceptronu tworzą wektor normalny, który określa prostą (w przypadku dwóch wejść) lub hiperpłaszczyznę decyzyjną. Trenowanie perceptronu to dopasowanie tej hiperpłaszczyzny do danych wejściowych, aby mógł wskazywać czy punkt należy lub nie należy do zbioru wskazywanego przez hiperpłaszczyznę. Dlatego tak ważne jest, aby dane były liniowo separowalne, inaczej dopasowanie do danych będzie niemożliwe.