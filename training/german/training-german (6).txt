1943 führten die Mathematiker Warren McCulloch und Walter Pitts das „Neuron“ als logisches Schwellwert-Element mit mehreren Eingängen und einem einzigen Ausgang in die Informatik ein.[1] Es konnte als Boolesche Variable die Zustände wahr und falsch annehmen und „feuerte“ (= wahr), wenn die Summe der Eingangssignale einen Schwellenwert überschritt (siehe McCulloch-Pitts-Zelle). Dies entsprach der neurobiologischen Analogie eines Aktionspotentials, das eine Nervenzelle bei einer kritischen Änderung ihres Membranpotentials aussendet. McCulloch und Pitts zeigten, dass durch geeignete Kombination mehrerer solcher Neuronen jede einfache aussagenlogische Funktion (UND, ODER, NICHT) beschreibbar ist.

1949 stellte der Psychologe Donald O. Hebb die Hypothese auf, Lernen beruhe darauf, dass sich die aktivierende oder hemmende Wirkung einer Synapse als Produkt der prä- und postsynaptischen Aktivität berechnen lasse.[2] Es gibt Anhaltspunkte, dass die Langzeit-Potenzierung und das sogenannte spike-timing dependent plasticity (STDP) die biologischen Korrelate des Hebbschen Postulates sind. Überzeugende Evidenz für diese These steht aber noch aus.

1957 schließlich publizierte Frank Rosenblatt das Perzeptron-Modell, das bis heute die Grundlage künstlicher neuronaler Netze darstellt.[3]